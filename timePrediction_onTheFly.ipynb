{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### IMPORT PACKAGES ###\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# to make pretty plots\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# to play with time :-)\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notebook can be run separately from the deliverable tool.\n",
    "if (('df' not in globals()) or ('df_test' not in globals())):\n",
    "    ### IMPORT THE DATA ###\n",
    "\n",
    "    # there are 3 datasets\n",
    "\n",
    "    # df is the test set, this is used to train the model\n",
    "    df = pd.read_csv('final data/BPI_Challenge_2012-training.csv')\n",
    "\n",
    "    # df_validation is the validation set, this is used to train the parameters, in our case the learningRate (how much our model should learn from a new event)\n",
    "    df_validation = pd.read_csv('final data/BPI_Challenge_2012-validation.csv')\n",
    "\n",
    "    # df_test is the unseen test ste, this is used to test how well the model works\n",
    "    df_test = pd.read_csv('final data/BPI_Challenge_2012-test.csv')\n",
    "\n",
    "\n",
    "    # Defining database-specific variables\n",
    "    case_column = \"case concept:name\"\n",
    "    event_column = \"event concept:name\"\n",
    "    timestamp_column = \"event time:timestamp\"\n",
    "    timeformat_timestamp = \"%Y-%m-%d %H:%M:%S.%f\"\n",
    "    lifecycle_column = 'event lifecycle:transition'\n",
    "\n",
    "    # set the row_number column\n",
    "    row_nr_column = 'row_nr'\n",
    "\n",
    "\n",
    "    # do you want to see the evaluation of the error?\n",
    "    errorEval = True\n",
    "    # do you want any error plots? (only available if errorEval == True)\n",
    "    printPlots = False\n",
    "    # do you want to write the plots to html? (only available if both errorEval and printPlots == True)\n",
    "    writePlots = False\n",
    "\n",
    "else:\n",
    "    # if you run this from the global tool, you won't see the evaluation anyways, so why compute it :-)\n",
    "    # but if you want, you can just set this to True\n",
    "    errorEval = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All columns are available in the dataframe\n",
      "   case concept:name      event concept:name     event time:timestamp  \\\n",
      "0             173688             A_SUBMITTED  2011-10-01 00:38:44.546   \n",
      "1             173688       A_PARTLYSUBMITTED  2011-10-01 00:38:44.880   \n",
      "2             173688           A_PREACCEPTED  2011-10-01 00:39:37.906   \n",
      "3             173688  W_Completeren aanvraag  2011-10-01 00:39:38.875   \n",
      "4             173691             A_SUBMITTED  2011-10-01 08:08:58.256   \n",
      "\n",
      "  event lifecycle:transition  row_nr  \n",
      "0                   COMPLETE       0  \n",
      "1                   COMPLETE       1  \n",
      "2                   COMPLETE       2  \n",
      "3                   SCHEDULE       3  \n",
      "4                   COMPLETE       4  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20182960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\20182960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "C:\\Users\\20182960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "# test if all columns are present:\n",
    "def checkIfColumnsArePresent (myDataFrame,listOfColumns):\n",
    "    \"\"\"\n",
    "    :param myDataFrame: the dataframe in which you want to check if the columns are present\n",
    "    :param listOfColumns: the list of columns that you want to check if they are present\n",
    "    :return: all Columns in the listOfColumns that are present in myDataFrame\n",
    "    \"\"\"\n",
    "\n",
    "    allColumns = set(myDataFrame.columns)\n",
    "    if not set(listOfColumns).issubset(allColumns):\n",
    "        print (f\"{' and '.join(set(listOfColumns).difference(allColumns))} are not available in the dataframe\")\n",
    "        return list(set(listOfColumns).intersection(allColumns))\n",
    "    else:\n",
    "        print (\"All columns are available in the dataframe\")\n",
    "        return listOfColumns\n",
    "\n",
    "\n",
    "\n",
    "# variables\n",
    "learningRate = 0.1\n",
    "\n",
    "# make a list of the columns you need\n",
    "columns = [case_column, event_column, timestamp_column]\n",
    "if lifecycle_column != None:\n",
    "    columns.append(lifecycle_column)\n",
    "\n",
    "# you can get rid of all the other columns, to make things faster\n",
    "# but you first have to check if they are there:\n",
    "# you only have to do this on one of the dataframes,\n",
    "# since they should all be subsets of the same big dataframe\n",
    "columns = checkIfColumnsArePresent (df,columns)\n",
    "\n",
    "dataset = df[columns]\n",
    "dataset_validation = df_validation[columns]\n",
    "dataset_test = df_test[columns]\n",
    "\n",
    "dataset[row_nr_column] = dataset.index\n",
    "dataset_validation[row_nr_column] = dataset_validation.index\n",
    "dataset_test[row_nr_column] = dataset_test.index\n",
    "\n",
    "print(dataset.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20182960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:1773: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "### ON THE FLY ###\n",
    "# This has two practical implications that we should be aware of\n",
    "# 1) You can't preprocess the data, when a new event comes in, you have to be able to process it on the spot\n",
    "# 2) You don't have any historical data, so your predictions can only rely on the things you have seen before\n",
    "\n",
    "# This means that the data needs to be sorted on timestamp_column\n",
    "# you need to first convert to a datetime, because otherwise you'll sort the strings\n",
    "# and since the day of the month is the first part of the string, you will first get all firsts of all months, etc.\n",
    "dataset.loc[:,timestamp_column] = pd.to_datetime(dataset.loc[:,timestamp_column], format=timeformat_timestamp)\n",
    "dataset = dataset.sort_values(by=timestamp_column)\n",
    "\n",
    "dataset_test.loc[:,timestamp_column] = pd.to_datetime(dataset_test.loc[:,timestamp_column], format=timeformat_timestamp)\n",
    "dataset_test = dataset_test.sort_values(by=timestamp_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### STORAGE ###\n",
    "# we want to keep track of things that happen within one instance (also called trace or case)\n",
    "\n",
    "# instanceStorage stores: key=instance, values= [event_column, timestamp_column, predictedDuration],\n",
    "# to look up the previous event in this instance, and it's predicted TimePassed\n",
    "instanceStorage = {}\n",
    "\n",
    "# we also want to keep track of the predictionError\n",
    "# errorStorage stores: key=unique integer, values= [previousEvent, previousTimestamp, predictionError, predictionErrorRatio, predictedTimePassed, timePassed]\n",
    "errorStorage = {}\n",
    "\n",
    "\n",
    "def updateStorage(case, event, event_base, timestamp, row_nr, predictedDuration):\n",
    "    \"\"\"\n",
    "    :param case: string; the case_id, so which instance this event belongs to\n",
    "    :param event: string; the id of the event we're evaluating\n",
    "    :param event_base: string; the name of the event as is in the original database\n",
    "    :param timestamp: pandas datetime; the time of the event that we're evaluating\n",
    "    :param row_nr: int; unique id in the dataset\n",
    "    :param predictedDuration: float; the predicted time for this event\n",
    "    (which you need to store to calculate the error when you know the real time passed when the next event comes in)\n",
    "    :return: previousEvent, timePassed; string, pandas datetime delta;\n",
    "    the id of the previous event and the time that passed since that event, you need this information to update the model\n",
    "    \"\"\"\n",
    "\n",
    "    # if this instance is already in the instanceStorage, we have a previous event\n",
    "    if case in instanceStorage:\n",
    "\n",
    "        # find the previous event\n",
    "        previousEvent = instanceStorage[case][0]\n",
    "        previousRow_nr = instanceStorage[case][3]\n",
    "\n",
    "        # calculate the time passed between the previous event and this event\n",
    "        thisTimestamp = pd.to_datetime(timestamp, format=timeformat_timestamp)\n",
    "        previousTimestamp = pd.to_datetime(instanceStorage[case][1], format=timeformat_timestamp)\n",
    "        timePassed = (thisTimestamp - previousTimestamp)/ datetime.timedelta(minutes=1)\n",
    "\n",
    "        # since we have also saved the predicted time between the previous event and this event,\n",
    "        # we can now also compute the difference\n",
    "\n",
    "        predictedTimePassed = instanceStorage[case][2]\n",
    "        if predictedTimePassed != None:\n",
    "            predictionError = abs(predictedTimePassed - timePassed)\n",
    "            if timePassed > 0:\n",
    "                predictionErrorRatio = predictionError/timePassed\n",
    "            else:\n",
    "                predictionErrorRatio = None\n",
    "        else:\n",
    "            predictionError = None\n",
    "            predictionErrorRatio = None\n",
    "            predictedTimePassed = None\n",
    "\n",
    "\n",
    "        # the key is now the row number of the event that is in the values\n",
    "        errorStorage[previousRow_nr] = [previousEvent, previousTimestamp, predictionError, predictionErrorRatio, predictedTimePassed, timePassed]\n",
    "\n",
    "    # otherwise we don't have a previous event\n",
    "    else:\n",
    "        previousEvent = None\n",
    "        timePassed = None\n",
    "\n",
    "    # key = case_column (so which case we're looking at), and the values are the activity and the timestamp\n",
    "    instanceStorage[case] = [event, timestamp, predictedDuration, row_nr, event_base]\n",
    "\n",
    "\n",
    "    return previousEvent, timePassed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### THE MODEL ###\n",
    "# We start with only one feature: the historical average time between two events based on the previous event\n",
    "# You don't need a second model for the test set, since you train your model on the training-set and then use it on the test-set\n",
    "\n",
    "predictionModel = {}\n",
    "\n",
    "def getPrediction(event):\n",
    "    \"\"\"\n",
    "    :param event: string; the name of the activity for which you want a prediction of time until the next event\n",
    "    :return: float (or None); predicted time until the next event in this instance, if there is already data on this activity,\n",
    "    otherwise None\n",
    "    \"\"\"\n",
    "\n",
    "    # if this activity is already in the predictionModel, then return the predicted duration\n",
    "    # try & except works slightly faster then if key in dict\n",
    "    # but only for valid keys, since you skip the if statement\n",
    "    try:\n",
    "        return predictionModel[event]\n",
    "\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "def updatePredictionModel(this_event_id, timePassed, learningRate):\n",
    "    \"\"\"\n",
    "    :param this_event_id: string; the slicing-id of the current event\n",
    "    :param timePassed: datatime delta; the time between the previous event in this instance and the current event\n",
    "    :param learningRate: float; how much weight should the new timePassed have in the update\n",
    "    :return: None\n",
    "    It will update the global variable predictionModel\n",
    "    \"\"\"\n",
    "\n",
    "    if timePassed != None:\n",
    "\n",
    "        # if this activity is already in the predictionModel, update the model\n",
    "        # try & except works slightly faster then if key in dict\n",
    "        # but only for valid keys, since you skip the if statement\n",
    "        try:\n",
    "            predictionModel[this_event_id] = timePassed*learningRate + predictionModel[this_event_id]*(1-learningRate)\n",
    "\n",
    "        except:\n",
    "            predictionModel[this_event_id] = timePassed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### RUNNING THE MODEL ###\n",
    "# We have to run the model line by line, we can't do things in parallel, otherwise it's not on the fly\n",
    "# This is the function that runs the model and outputs the predictedTimeStamp for the next event\n",
    "\n",
    "def processEvent(case, event, additional, timestamp, row_nr, includeAdditional=True, returnPrediction=False):\n",
    "    \"\"\"\n",
    "    :param case: string; the case_id, so which instance this event belongs to\n",
    "    :param event: string; the id of the event we're evaluating\n",
    "    :param additional: string or number; something additional that you want to slice on\n",
    "    :param timestamp: pandas datetime; the time of the event that we're evaluating\n",
    "    :param row_nr: int; unique id in the dataset\n",
    "    :param includeAdditional: bool, if you want to include additional information to slice on or you want the baseline model\n",
    "    :return: None\n",
    "    It updates the global variables predictionModel, instanceStorage and errorStorage\n",
    "    \"\"\"\n",
    "\n",
    "    # step 1: preProcess the incoming event -> create an event_id that indicates the dataSlice that you want to use\n",
    "    # The event-id is where you want to slice the data on before you take the average timePassed\n",
    "    # If you don't want to include any additional information, this is just the event itself\n",
    "    # otherwise include it in the event_id to make sure that you slice on everything you want to slice on\n",
    "    if includeAdditional == True:\n",
    "\n",
    "        howManyConcurentEvents = [x[4] for x in instanceStorage.values()].count(event)\n",
    "\n",
    "        #howManyConcurentEvents = [x[0].split('_XX_')[0] for x in instanceStorage.values()].count(event)\n",
    "        if howManyConcurentEvents > 3:\n",
    "            # when an event is the last event in an instance, it won't get out of the instance storage\n",
    "            # so this number can get really high for events that are common to be the last one\n",
    "            # even though they are not actually running at the same time. Therefore I will top it off at 3.\n",
    "            howManyConcurentEvents = 3\n",
    "        event_id = event+'_XX_'+additional+'_XX_'+str(howManyConcurentEvents)\n",
    "\n",
    "    else:\n",
    "        event_id = event\n",
    "\n",
    "\n",
    "    # step 2: predict\n",
    "    # look up if this event is already in predictionModel\n",
    "    # if it is, use the predictedDuration as current prediction\n",
    "    # otherwise predict: unknown\n",
    "    predictedDuration = getPrediction(event_id)\n",
    "\n",
    "    # step 3: loop up historical data\n",
    "    # you only know things of the past, so you want to check if there is a previous event in the instanceStorage\n",
    "    # if there is, you can calculate the duration between that event and this one\n",
    "    previousEvent, timePassed = updateStorage(case, event_id , event, timestamp, row_nr, predictedDuration)\n",
    "\n",
    "    # step 4: update predictionModel\n",
    "    updatePredictionModel(previousEvent, timePassed, learningRate)\n",
    "\n",
    "    # step 5: return the prediction of the next timestamp\n",
    "    if returnPrediction == True:\n",
    "        try:\n",
    "            # we can't predict the next timestamp if we don't have enough historical data yet to make prediction of the timeDelta\n",
    "            # so if the timeDelta is still None, we will have to return None\n",
    "            return timestamp+datetime.timedelta(minutes=predictedDuration)\n",
    "        except:\n",
    "            return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict: Elapsed time during the training the model in seconds: 73.515625\n"
     ]
    }
   ],
   "source": [
    "### TRAIN THE MODEL ON THE TRAIN DATASET ###\n",
    "# We have to run the model line by line, we can't do things in parallel, otherwise it's not on the fly\n",
    "# This actually runs the model\n",
    "\n",
    "### Run over all lines =>\n",
    "# Start the stopwatch / counter\n",
    "\n",
    "start_time = time.process_time()\n",
    "\n",
    "# running the on-the-fly model for each row\n",
    "# I have tested it, ... for ... in ... does iterate over all events in order\n",
    "# so we're not using information from the future.\n",
    "df[\"OnTheFly Prediction for Next Timestamp\"] = [processEvent(case, event, additional, timestamp, row_nr, includeAdditional=True, returnPrediction=True) for case,event,additional,timestamp,row_nr in zip(dataset[case_column],dataset[event_column], dataset[additionalInfo], dataset[timestamp_column], dataset[row_nr_column])]\n",
    "\n",
    "# Stop the stopwatch / counter\n",
    "end_time = time.process_time()\n",
    "\n",
    "print(\"dict: Elapsed time during the training the model in seconds:\", end_time-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute error on the training-set is: 649.94 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\20182960\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: RuntimeWarning: divide by zero encountered in log2\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "if errorEval == True:\n",
    "\n",
    "    ### THE PREDICTION ERROR ###\n",
    "    # plot different things to get more insight\n",
    "    # first the difference in time passed\n",
    "    # then the log of the difference in time passed\n",
    "    # then the ratio between the difference in time passed and the real time passed\n",
    "\n",
    "    # flatten the error dictionary to a dataframe\n",
    "    error_df = pd.DataFrame.from_dict(errorStorage, orient='index', columns=['event_column', 'timestamp', 'predictionError', 'predictionErrorRatio', 'predictedTimePassed', 'timePassed'])\n",
    "    error_df.loc[:,'timestamp'] = pd.to_datetime(error_df.loc[:,'timestamp'])\n",
    "    error_df = error_df.sort_values(by='timestamp')\n",
    "    error_df['event_column_basic'] = error_df.event_column.str.split('_XX_').str.get(0)\n",
    "\n",
    "    # calculate the log of the prediction error\n",
    "    error_df['log_predictionError'] = [np.log2(x) for x in error_df['predictionError']]\n",
    "\n",
    "    # calculate statistics\n",
    "    mae = np.mean(error_df['predictionError'])\n",
    "    print('The mean absolute error on the training-set is: ' + str(round(mae, 2)) + ' minutes.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if errorEval == True:\n",
    "\n",
    "\n",
    "    if printPlots == True:\n",
    "\n",
    "        ### HISTOGRAMS ###\n",
    "\n",
    "        # make the prediction error plot\n",
    "        fig= px.line(error_df, x='timestamp', y='predictionError', color='event_column')\n",
    "        fig.update_layout(title='Prediction error (difference in timePassed between real timePassed an predicted timePassed) per activity')\n",
    "        fig.show()\n",
    "        if writePlots == True:\n",
    "            fig.write_html(\"images/predictionError_withboth.html\")\n",
    "\n",
    "        # make the log prediction error plot\n",
    "        fig= px.line(error_df, x='timestamp', y='log_predictionError', color='event_column')\n",
    "        fig.update_layout(title='Prediction error ratio (log of the difference in timePassed) per activity')\n",
    "        fig.show()\n",
    "        if writePlots == True:\n",
    "            fig.write_html(\"images/log_predictionError_withboth.html\")\n",
    "\n",
    "        # make the ratio prediction error plot\n",
    "        fig= px.line(error_df, x='timestamp', y='predictionErrorRatio', color='event_column')\n",
    "        fig.update_layout(title='Prediction error ratio (difference in timePassed divided by real timePassed) per activity')\n",
    "        fig.show()\n",
    "        if writePlots==True:\n",
    "            fig.write_html(\"images/predictionErrorRatio_withboth.html\")\n",
    "\n",
    "\n",
    "        ### BOXPLOTS ###\n",
    "        fig = px.box(error_df, y='predictionError', color='event_column_basic')\n",
    "        fig.show()\n",
    "        if writePlots == True:\n",
    "            fig.write_html('images/box_predictionError_withboth.html')\n",
    "\n",
    "        fig = px.box(error_df, y='predictionErrorRatio', color='event_column_basic')\n",
    "        fig.show()\n",
    "        if writePlots == True:\n",
    "            fig.write_html('images/box_predictionErrorRatio_withboth.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if errorEval == True:\n",
    "\n",
    "    if printPlots == True:\n",
    "\n",
    "        eventList = np.unique(error_df['event_column_basic'])\n",
    "\n",
    "        for activity in eventList:\n",
    "            fig=px.box(error_df[error_df['event_column_basic'] == activity], y='predictionError', color='event_column')\n",
    "            fig.show()\n",
    "            if writePlots == True:\n",
    "                fig.write_html('images/box_predictionError_splitForConcurentEvents.html')\n",
    "\n",
    "    # there is a difference in error distribution, so it looks like it does have an effect.\n",
    "    # I don't save the timePassed anywhere, so I can't check on that. But in the InfluenceOnTime file I have tested that there is indeed a\n",
    "    # different distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict: Elapsed time during the setting of the parameters on the validation set took  19.9375 seconds.\n",
      "The new learning rate is  0.01\n"
     ]
    }
   ],
   "source": [
    "### SET THE PARAMETERS ON THE VALIDATION SET ###\n",
    "\n",
    "\n",
    "learningRateList = [0.01, 0.1, 0.5]\n",
    "maeList = []\n",
    "\n",
    "# Start the stopwatch / counter\n",
    "start_time = time.process_time()\n",
    "\n",
    "for learningRate in learningRateList:\n",
    "    # You can use the model that you have created during the train, so you don't have to start with empty model\n",
    "    # but you do have to start with an empty storage\n",
    "    instanceStorage = {}\n",
    "    errorStorage = {}\n",
    "\n",
    "    # running the on-the-fly model for each row\n",
    "    # I have tested it, ... for ... in ... does iterate over all events in order\n",
    "    # so we're not using information from the future.\n",
    "    df_validation[\"OnTheFly Prediction for Next Timestamp\"] = [processEvent(case, event, additional, timestamp, row_nr, includeAdditional=True, returnPrediction=True) for case,event,additional,timestamp,row_nr in zip(dataset_validation[case_column],dataset_validation[event_column], dataset_validation[additionalInfo], dataset_validation[timestamp_column], dataset_validation[row_nr_column])]\n",
    "\n",
    "    error_df_validation = pd.DataFrame.from_dict(errorStorage, orient='index', columns=['event_column', 'timestamp', 'predictionError', 'predictionErrorRatio', 'predictedTimePassed', 'timePassed'])\n",
    "    maeList.append(np.mean(error_df_validation['predictionError']))\n",
    "\n",
    "# let the learningRate to the learningRate with the lowest mae\n",
    "learningRate = learningRateList[np.argmin(maeList)]\n",
    "\n",
    "# Stop the stopwatch / counter\n",
    "end_time = time.process_time()\n",
    "\n",
    "print(\"dict: Elapsed time during the setting of the parameters on the validation set took \", end_time-start_time , \"seconds.\")\n",
    "print(\"The new learning rate is \", learningRate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict: Elapsed time during the whole program on the testset in seconds: 5.140625\n"
     ]
    }
   ],
   "source": [
    "### RUN ON THE TEST SET ###\n",
    "# You can use the model that you have created during the train, so you don't have to start with empty model\n",
    "# but you do have to start with an empty storage\n",
    "\n",
    "instanceStorage = {}\n",
    "errorStorage = {}\n",
    "\n",
    "# Start the stopwatch / counter\n",
    "start_time = time.process_time()\n",
    "\n",
    "# running the on-the-fly model for each row\n",
    "# I have tested it, ... for ... in ... does iterate over all events in order\n",
    "# so we're not using information from the future.\n",
    "df_test[\"OnTheFly Prediction for Next Timestamp\"] = [processEvent(case, event, additional, timestamp, row_nr, includeAdditional=True, returnPrediction=True) for case,event,additional,timestamp,row_nr in zip(dataset_test[case_column],dataset_test[event_column], dataset_test[lifecycle_column], dataset_test[timestamp_column], dataset_test[row_nr_column])]\n",
    "\n",
    "# Stop the stopwatch / counter\n",
    "end_time = time.process_time()\n",
    "\n",
    "print(\"dict: Elapsed time during the whole program on the testset in seconds:\", end_time-start_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean absolute error on the test-set is: 573.57 minutes.\n"
     ]
    }
   ],
   "source": [
    "if errorEval == True:\n",
    "\n",
    "    ### THE PREDICTION ERROR ###\n",
    "    # plot different things to get more insight\n",
    "    # first the difference in time passed\n",
    "    # then the log of the difference in time passed\n",
    "    # then the ratio between the difference in time passed and the real time passed\n",
    "\n",
    "    # flatten the error dictionary to a dataframe\n",
    "    error_df_test = pd.DataFrame.from_dict(errorStorage, orient='index', columns=['event_column', 'timestamp', 'predictionError', 'predictionErrorRatio', 'predictedTimePassed', 'timePassed'])\n",
    "    error_df_test.loc[:,'timestamp'] = pd.to_datetime(error_df_test.loc[:,'timestamp'])\n",
    "    error_df_test = error_df_test.sort_values(by='timestamp')\n",
    "    error_df_test['event_column_basic'] = error_df_test.event_column.str.split('_XX_').str.get(0)\n",
    "\n",
    "    # calculate the log of the prediction error\n",
    "    error_df_test['log_predictionError'] = [np.log2(x) for x in error_df_test['predictionError']]\n",
    "\n",
    "    # calculate statistics\n",
    "    mae = np.mean(error_df_test['predictionError'])\n",
    "    print('The mean absolute error on the test-set is: ' + str(round(mae, 2)) + ' minutes.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if errorEval == True:\n",
    "    ### BOXPLOTS ###\n",
    "\n",
    "    if printPlots == True:\n",
    "        fig = px.box(error_df_test, y='predictionError', color='event_column_basic', title='PredictionError on the test-dataset')\n",
    "        fig.show()\n",
    "        if writePlots == True:\n",
    "            fig.write_html('images/TEST_box_predictionError_withboth.html')\n",
    "\n",
    "\n",
    "        fig = px.box(error_df_test, y='predictionErrorRatio', color='event_column_basic', title='PredictionErrorRatio on the test-dataset')\n",
    "        fig.show()\n",
    "        if writePlots == True:\n",
    "            fig.write_html('images/TEST_box_predictionErrorRatio_withboth.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
